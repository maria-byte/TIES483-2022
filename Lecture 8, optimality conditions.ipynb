{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 8, Optimality conditions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are still studying the full problem\n",
    "\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What aspects are important for optimality conditions?\n",
    "* Think about this for a while\n",
    "* You can first think about the case without constraints and, then, what should be added there\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimality conditions for **unconstrained** optimization\n",
    "\n",
    "* (**Necessary condition**) Let $f$ be twice differentiable at $x^*\\in\\mathbb R^n$. If $x^*$ is a local minimizer, then $\\nabla f(x^*)=0$ and the Hessian matrix $H(x^*)$ is positively semidefinite.\n",
    "* (**Sufficient condition**) Let $f$ be twice continuously differentiable at $x^*\\in\\mathbb R^n$. If $\\nabla f(x^*)=0$ and $H(x^*)$ is positively definite, then $x^*$ is a strict local minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to identify which points are optimal, we want to define similar conditions as there are for unconstrained problems through the gradient:\n",
    "\n",
    ">If $x$ is a  local optimum to function $f$, then $\\nabla f(x)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Karush-Kuhn-Tucker (KKT) conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem (First order Karush-Kuhn-Tucker (KKT) Necessary Conditions)** \n",
    "\n",
    "Let $x^*$ be a local minimum for problem\n",
    "$$\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us assume that objective and constraint functions are continuosly differentiable at a point $x^*$ and assume that $x^*$ satisfies some regularity conditions (see e.g., https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_.28or_constraint_qualifications.29 ). Then there exists unique Lagrance multiplier vectors $\\mu^*=(\\mu_1^*,\\ldots,\\mu_J^*)$ and $\\lambda^* = (\\lambda^*_1,\\ldots,\\lambda_K^*)$ such that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL(x^*,\\mu^*,\\lambda^*) = 0\\\\\n",
    "&\\mu_j^*\\geq0,\\text{ for all }j=1,\\ldots,J \\text{  (also known as **Dual feasibility**)}\\\\\n",
    "&\\mu_j^*g_j(x^*)=0,\\text{for all }j=1,\\ldots,J,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $L$ is the *Lagrangian function* $$L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x)$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Lagrangian Function can be viewed as a function aggregated the original objective function plus the **penalized terms on constraint violations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An example of constraint qualifications for inequality constraint problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Definition (regular point)**\n",
    "\n",
    "A point $x^*\\in S$ is *regular* if the set of gradients of the active inequality constraints \n",
    "\n",
    "$$\n",
    "\\{\\nabla g_j(x^*) | \\text{ constraint } i \\text{ is active}\\}\n",
    "$$\n",
    "\n",
    "is linearly independent. This means that none of them can be expressed as a linear combination of the others. (*In a simple language one might say that they point to different directions; as an example you can think of the basis vectors of $\\mathbb R^n$*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "KKT conditions were developed independently by \n",
    "* William Karush:\"Minima of Functions of Several Variables with Inequalities as Side Constraints\". *M.Sc. Dissertation*, Dept. of Mathematics, Univ. of Chicago, 1939\n",
    "* Harold W. Kuhn &  Albert W. Tucker: \"Nonlinear programming\", In: *Proceedings of 2nd Berkeley Symposium*, pp. 481â€“492, 1951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The coefficients $\\mu$ and $\\lambda$ are called the *KKT multipliers*.\n",
    "\n",
    "The first equality \n",
    "\n",
    "$$\n",
    "\\nabla_xL(x,\\mu,\\lambda) = 0\n",
    "$$\n",
    "\n",
    "is called the stationary rule and the requirement \n",
    "\n",
    "$$\n",
    "\\mu_j^*g_j(x)=0,\\text{for all }j=1,\\ldots,J\n",
    "$$\n",
    "\n",
    "is called the complementarity rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "* In some cases, the necessary conditions are also sufficient for optimality.\n",
    "\n",
    "* For example, the necessary conditions mentioned above are sufficient for optimality if $f$, $g_j$ and $h_k (\\forall j, k)$ are convex (in a minimization problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Consider the optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min &\\qquad (x_1^2+x^2_2+x^2_3)\\\\\n",
    "\\text{s.t}&\\qquad x_1+x_2+x_3-3\\geq 0.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us verify the KKT necessary conditions for the local optimum $x^*=(1,1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that\n",
    "\n",
    "$$\n",
    "L(x,\\mu,\\lambda) = (x_1^2+x_2^2+x_3^2)-\\mu_1(x_1+x_2+x_3-3)\n",
    "$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$\n",
    "\\nabla_x L(x,\\mu,\\lambda) = (2x_1-\\mu_1,2x_2-\\mu_1,2x_3-\\mu_1)\n",
    "$$\n",
    "\n",
    "and if $\\nabla_x L([1,1,1],\\mu,\\lambda)=0$, then \n",
    "\n",
    "$$\n",
    "2-\\mu_1=0 $$\n",
    "which holds when $$\n",
    "\\mu_1=2.\n",
    "$$\n",
    "\n",
    "In addition to this, we can see that $x^*_1+x^*_2+x^*_3-3= 0$. Thus, the completementarity rule holds even though $\\mu_1\\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us check the KKT conditions for a solution that is not a local optimum. Let us have $x^*=(0,1,1)$.\n",
    "\n",
    "$$\n",
    "\\nabla_x L(x,\\mu,\\lambda) = (2x_1-\\mu_1,2x_2-\\mu_1,2x_3-\\mu_1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can easily see that in this case, the conditions are \n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{array}{c}\n",
    "-\\mu_1 = 0\\\\\n",
    "2-\\mu_1=0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Clearly, there does not exist a $\\mu_1\\in \\mathbb R$ such that these equalities would hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us check the KKT conditions for another solution that is not a local optimum. Let us have $x^*=(2,2,2)$.\n",
    "\n",
    "$$\n",
    "\\nabla_x L(x,\\mu,\\lambda) = (2x_1-\\mu_1,2x_2-\\mu_1,2x_3-\\mu_1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can easily see that in this case, the conditions are\n",
    "\n",
    "$$\n",
    "4-\\mu_1 = 0\n",
    "$$\n",
    "\n",
    "Now, $\\mu_1=4$ satisfies this equation. However, now\n",
    "\n",
    "$$\n",
    "\\mu_1(x^*_1+x^*_2+x^*_3-3)=4(6-3) = 12 \\neq 0.\n",
    "$$\n",
    "\n",
    "Thus, the completementarity rule fails and the KKT conditions are not true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example\n",
    "\n",
    "Formulate the KKT conditions for the following example:\n",
    "$$\n",
    "min ð‘“(\\mathbf{x}) = (ð‘¥_1 âˆ’ 3)^2 + (ð‘¥_2 âˆ’ 2)^2\\\\\n",
    "s.t. \\\\\n",
    "ð‘¥_1^2 + ð‘¥_2^2 â‰¤ 5,\\\\\n",
    "ð‘¥_1 + 2ð‘¥_2 = 4,\\\\\n",
    "ð‘¥_1, ð‘¥_2 â‰¥ 0\n",
    "$$\n",
    "\n",
    "Check them for $x^* = (2,1)$\n",
    "\n",
    "* This part will be completed during the lecture by students (10 min).\n",
    "* You need to use both conditions to find the KKT multipliers. There are three inequality constraints (j=1,2,3) and one equality constraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### A reminder of KKT necessary conditions:\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL(x^*,\\mu^*,\\lambda^*) = 0\\\\\n",
    "&\\mu_j^*\\geq0,\\text{ for all }j=1,\\ldots,J\\\\\n",
    "&\\mu_j^*g_j(x^*)=0,\\text{for all }j=1,\\ldots,J,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $L$ is the *Lagrangian function* $$L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometric interpretation of the KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stationary rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the *Lagrangian function* L as:  $$L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x)$$.\n",
    "\n",
    "The stationary rule is:\n",
    "$$\n",
    "\\nabla_xL(x,\\mu,\\lambda) = 0\n",
    "$$\n",
    "\n",
    "The stationary rule can be written as: There exist $\\mu,\\lambda'$ so that\n",
    "\n",
    "$$\n",
    "-\\nabla f(x) = -\\sum_{j=1}^K\\mu_j\\nabla g_j(x) + \\sum_{k=1}^K\\lambda'_k\\nabla h_k(x).\n",
    "$$\n",
    "\n",
    "Notice that we have slightly different $\\lambda'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, remember that the $-\\nabla v(x)$ gives us the direction of reduction for a function $v$.\n",
    "\n",
    "Thus, the above equation means that the direction of reduction of the function $-\\nabla f(x)$ is countered by the direction of the reduction of the inequality constraints $-\\nabla g_j(x)$ and the directions of either growth (or reduction, since $\\lambda'$ can be negative) of the equality constraints $\\nabla h_k(x)$.\n",
    "\n",
    "**This means that the function cannot get reduced without reducing the inequality constraints (making the solution infeasible, if already at the bound), or increasing or decreasing the equality constraints (making, thus, the solution again infeasible).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### With just one inequality constraint this means that the negative gradients of $f$ and $g$ must point to the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![alt text](images/KKT_inequality_constraints.svg \"KKT with inequality constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### With equality constraints this means that the negative gradient of the objective function and the gradient of the equality constraint must either point to the same or opposite directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/KKT_equality_constraints.svg \"KKT with inequality constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Complementarity conditions\n",
    "Another way of expressing complementarity condition\n",
    "\n",
    "$$\n",
    "\\mu_jg_j(x) = 0 \\text{ for all } j=1,\\ldots,J\n",
    "$$\n",
    "\n",
    "is to say that both $\\mu_j$ and $g_j(x)$ cannot be positive at the same time. Especially, if $\\mu_j>0$, then $g_j(x)=0$.\n",
    "\n",
    "**This means that if we want to use the gradient of a constraint for countering the reduction of the function, then the constraint must be at the boundary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient conditions:\n",
    "\n",
    "* The necessary conditions are sufficient for optimality if $f$, $g_j$ and $h_k (\\forall j, k)$ are convex (in a minimization problem).\n",
    "\n",
    "* In general, the necessary conditions are not sufficient for optimality and additional information is required, e.g., the Second Order Sufficient Conditions for smooth functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Second-order sufficient conditions (Projected Hessian is positive definite)\n",
    "\n",
    "For a smooth, non-linear optimization problem, a second order sufficient condition is given as follows:\n",
    "\n",
    "If $(x^*, \\mu^*, \\lambda^*$ be a constrained local minimum for the Lagrangian function\n",
    "\n",
    "$$L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x)$$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$ d^T \\nabla _{\\mathbf{xx}}^2L(x^*,\\mu^*,\\lambda^*) d > 0  \\text {         (Hessian is positive definite) }$$ \n",
    "\n",
    "\n",
    "But in constrained optimization we are **not interested in all d**.\n",
    "\n",
    "Instead, we are looking for the $d$ vectors that lies on the tangent space (active constraints).\n",
    "\n",
    "\n",
    "* i.e., $$ \\forall d \\neq 0 \\text{,   } [\\nabla _{x}g_{j}(x^{*}),\\nabla _{x}h_{k}(x^{*})]^Td = 0 \\text{;   } \\forall j, k.$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
